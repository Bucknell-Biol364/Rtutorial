---
title: "Rtutorial"
output: html_document
author: "Ian Vogel, Julie Berhane"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

R Tutorial for Animal Behavior 296 and Neuroscience 253 at Bucknell University

R Basics:

Welcome to R for Neuro 253!

R is a program for data science. It is an extremely useful tool for manipulating, visualizing, and investigating data. It will allow you to explore data to formulate hypotheses and then employ statistical tests to assess them.

Before diving into the tutorial, we want to begin by mentioning that the internet can be a great resource for learning R and finding code from other people who have done similar analyses. You can master R to the point where you are able to freely code and create your own functions, but for the most part everything you want to do with R has been done before and this code can be easily accessed online. This tutorial will teach you the basics, but we encourage you to use the internet to supplement your learning if you want to do more complicated analyses or visualizations.

Just to give you the lay of the land, the window you are reading this in is where you will write text and code to work with data. Below is the console where code is executed. In the upper right is a window that will show your environment and history of edits. Finally in the bottom right is where you will find something called your working directory (Files tab), any graphs you create (Plots tab), packages that you have installed and that are included in the system library (Packages tab), and where inquires made by typing ? before a function or package name will be answered (Help tab). You can ignore the Viewer tab for now.

Before we can do any data visualizations or analyses we will first need a place to store all of our data sets as well as the code we write to work with them. This place is called your working directory and you will need to create one for this exercise. 

To create a working directory you should first create a folder in a location on this computer that you can readily access. We recommend creating the folder in Documents. Next select the "Session" tab located above this window near where it says "File" and "Edit". Next click "Set Working Directory" and select "Choose Directory..." from the drop down menu. Within the file explorer that will appear, navigate to where you stored the new folder you just created. Select the folder and click "Ok" to set it as your working directory. In this case you will want to move the folder you cloned from github to your newly established working directory so that you can readily access the files you will need.

You can also set the working directory with code using the function below and typing out the full file path.

```{r}
# switch code to setwd("~/Documents/WorkingDirectoryFolderName")
# If you left the zip file in your Downloads folder, change Documents to be Downloads
setwd("~/Documents/Rtutorial")
```

Next we want some data to play with! There are a ton of great sample data sets avalable online and even a few built in ones here in R, but for this course we will be using a data set from a lab similar to those you will be performing this semester. The data set "Neuro_253_Cleaned_Data" is a series of extracellular neuronal recordings from crayfish. It includes recordings of "spikes" which refers to the firing of the neurons in the units of action potentials per second measured both before and after treatment with drugs. 

This file is in the .csv format, an abbreviation for "comma separated values". You can create such a file from an excel document. After entering and organizing data in an excel spreadsheet you can "save as" with the file extension .csv. You will need to save your data sets within the folder you have set as your working directory so that it will be accessible within R. After you have moved all of the cloned files to your working directory you should see the "Neuro_253_Cleaned_Data" dataset within the directory.

Once the Neuro_253_Cleaned_Data.csv file is in your working directory you will want to read it into the R session as a data frame. A data frame is a table of information that can be of various types which is convenient when data are not of uniform classes. The read.csv function automatically takes the file name you provide in quotes and creates a data frame named whatever you provide to the left of the command containing the information from your data file. Below you can see the code in action.

```{r Import Data}
Neuron_Recording_Data <- read.csv("Neuro_253_Cleaned_Data.csv")
View(Neuron_Recording_Data)
```

You now have a data set in R that is ready to be manipulated using the commands within this document. 

General Syntax:

The '<-' symbol incdicates assignment and allows you to create a variable (for instance in the previous code you can see that we used this to create the data frame for our data set)

To access a specific variable you can use the dollar sign. For instance to select a specific drug within this data set you would want to use Neuron_Recording_Data$drug name. 

R has many built in functions available that can be used to perform some sort of action on existing variables. To perform a function on a variable you use the syntax function(variable).




Data Exploration:

You can begin exploring your data set by looking at the head (first five rows) and tail (last five rows) of your dataset. You can also view the full dataset by typing the name of the file and running that line of code.

Another issue you should determine from the start is if there are missing values in your dataset. If it is a dataset you have collected yourself this may be easy to determine if the dataset was small enough, but when using large data or data from another source you should make sure to check this. You can check for missing values (often represented as NA, but could potentially be -999 or something else) and then choose if you want to keep or omit them from the data set.

Here is some generic code for exploring a data set named DataFile. Now type them in yourself in the order they appear by changing the word "Datafile" to "Neuro_253_Cleaned_Data.csv".

```{r}
# Display full dataset
DataFile
# Display first five rows only (useful for large datasets)
head(DataFile)
# Display last five rows only
tail(DataFile)
# Count missing values
sum(is.na(DataFile))
# Exclude observations with missing values
NewDataFileName <- na.omit(DataFile)
```

The layout of the data can also have consequences for how well your R code works. R treats the data as if each row is from a single observation. For example, let's consider you were determining if there is a difference test scores in two separate classes. The data could be set up such that Class A is the title of the first column and below it are the test scores of 10 different students, and Class B is the title of the second column with it's students test scores below it. R would treat the first row as one observation when it is actually two, one test score from class A and one score from test B. Stacked data will have the title of the first column be Grades or Values, and the title of the second column Class. So the rows would then be set up as a single observation for a single student, showing their test score and the Class (A or B) that they belong to.

You can use the stack() function to stack all of your data or stack data with particular values. You will not need to use this function for this tutorial but it is still good to know if you encounter unstacked data in future data sets.

```{r}
DataFileStacked <- stack(DataFile)
DataFileStackere <- stack(VariableA, select=c("Variable B value 1", "Variable B value 2"))
```

Once the data is set up how you want, you can look at some quick summary statistics. The mean and middle are different ways to look at the middle of the data set. You can also look at the dispersion of the data with the range, standard deviation, variance, standard error of the mean, and interquartile range. When trying to find the mean, median, etc. of a specific variable you must first specify the dataset you are looking at and then the variable you want to focus on. This is done in the format DataFile$Variable.

```{r}
# mean
mean(DataFile$Variable)
# median
median(DataFile$Variable)
# range
range(DataFile$Variable)
# standard deviation
sd(DataFile$Variable)
# variance
var(DataFile$Variable)
# standard error of the mean. sqrt is the square root function. length returns the number of values in that variable
SEM = sd(DataFile$Variable)/sqrt(length(DataFile$Variable))
# interquartile range
IQR(DataFile$Variable)
```

Try these commands with your dataset and pay close attention to what it tells you about this dataset in particular. What does it mean if the data has a large range? a small interquartile distance? Are the mean and median similar?

When working with quantitative data you should check to see if it is normally distributed. This is important when you eventually decide which statistical test to use as some of them assume that the data is normal. 

You can check to see if the data is normal using the simple.eda function in the UsingR package. The simple.eda function will show you a histogram, boxplot, and normal Q-Q plot of the data. 

A histogram will give you an idea of the distrbution of your variable by showing the frequency of each observed value. The graph should follow a bell shaped curve if the data is normal. 

A boxplot shows the spread of the data and if there are any outliers. Normal data should only have 1 outlier for every 100 data points. So if you have a small data set but there are a lot of outliers (points that extend beyond the box and whiskers) in your boxplot it may not be normally disributed. 

Q-Q plots are used to determine if the distribution of the data matches a normally distributed data set. If the points do not closely lie on the regression the plot creates then it is most likely not normally distributed.

```{r}
install.packages("UsingR")
library(UsingR)
simple.eda(Neuron_Recording_Data)
```

Try the simple.eda function with your neuro data set. Does the distribution appear normal - does the histogram show the characteristic bell shape? In this case the output will be a little funky because of the way the data is organized. Instead of the three different graph types we would normally expect, we instead get an output of a stragne graph and three histograms because we did not indicate a variable to inspect, so each graph depicts the distribution of one column from the dataframe. 

The first two graphs are not all that helpful as they just reveal that there were 5 runs for pilocarpine and nicotine,but only four for carbachol. 

The last two graphs show the distributions for control and experimental spikes respectively. Let's look a little closer at teh control spikes data. 

Try the simple.eda function again, but this time with just the control spikes recordings.
Hint: remember the selection syntax we discussed earlier "dataset$variable"." The variable here being the column name we are interested in

```{r}
simple.eda()
```


Do the control spikes data look normal? Although graphs depicting distributions can be helpful for giving an overview of potential normality, we want to be completely confident on whether the data set is normal.


The Shapiro Wilks test is a way to statistically test if data is normal. This can be done using the shapiro.test function. The null hypothesis for this test is that the data is normal, so a p value less than 0.05 will reject the null and assume that the data is not normal. Therefore, if the p value returned is greater than 0.05 then the data is normal. This function will return some output that you don't need to look at. To focus on the p value use shapiro.test(DataFile_Variable)$p.value.

```{r}
shapiro.test(DataFile$Variable)$p.value
```

If the data is not normal, you may want to see if the data can be transformed and fit to be normal. This can be done by taking the log of the data. You can take the natural log and the log base 10 of the data and then run a Shapiro Wilks test to see if it is now normal.

```{r}
# natural log
shapiro.test(log(DataFile$Variable))
# log base 10
shapiro.test(log10(DataFile$Variable))
```

If the data still isn't normal after transforming, then you should consider it not normal and then use non-parametric tests during your statistical analysis. Parametric tests (such as the t test and ANOVA) are those that assume that data is normal. These are not suitable for analyzing non-normal data and you will have to determine which non-parametric test is appropriate when you begin to run statistical tests on your data.

Data Visualization:

Although statistical tests will determine if there is a statistically significant relationship between your variables, it can be easier to convey this information visually for the reader.

There are many ways to visually represent data, but you should select one based on what is appropriate for the types of variables you are testing as well as how many variables. The variable you are testing will either be numeric or categorical (factor). You should also make decisions based on if your numeric variable is discrete or continuous.

The zip folder downloaded for this tutorial includes a helpful guide (KylaJacksonPlotTypes.png) for determining which visualization is best given your variables of interest.

Below is the code for for different graphs you can make in R.

```{r}
# Bar graph/plot
Height = c(mean(x),mean(y),mean(z))
barplot(Height, xlab = "My x-axis label", ylab = "My y-axis label", names = c("Name of x", "Name of y", "Name of z"), cex.lab = 1.5)
abline(h=0)

# Grouped bar graph
install.packages("ggplot2")
library(ggplot2)
ggplot(data, aes(fill=condition, y=value, x=specie))
geom_bar(position="dodge", stat="identity")

# Histogram
hist(variable, cex.lab = 1.5, main ="Main Title", breaks = 34, ylim = c(0,10))
abline(v = mean(variable), lwd = 5)
par(mfrow = c(1,1))

# Scatterplot
plot(dataset, xlim = c(0.10), ylim = c(0,10), xlab = "My x-axis label", ylab = "My y-axis label", main = "Custom Title", pch = 16, cex, 1.2, cex.lab = 1.5)

# Mosaic plot
ggplot(data = dataset) +
   geom_mosaic(aes(x = product(variable1, variable2), fill=variable1), na.rm=TRUE) + 
  # OR
mosaicplot(~ quantVariable + factorVariable, data = dataset, color = TRUE)

# Strip chart
ggplot(dataset) +
  aes(x = as.numeric(factorVariable), y = quantVariable) +
  geom_jitter(aes(color = factor(otherVariable)), na.rm=TRUE) +
  theme_cowplot() +
  theme(legend.position = "right") +
  labs(color="Legend title") + 
  xlab("My x-axis label for factor variable") + 
  ylab("My y-axis label for quant variable")

# Box plot, x,y,z are different factors of one variable
boxplot(x,y,z, names = c("Name1","Name2","Name3"), xlab = "My x-axis label", ylab = "My y-axis label", cex.lab = 1.5, ylim = c(0,max(c(x,y,z))))
# OR
# Box plot for one numeric and one categorical variable
boxplot(NumericVariable ~ CategoricalVariable, data = DataFile, ylab = "My y-axis label", xlab = "My x-axis label", main = Title)
```

As we mentioned before there are many sample data sets built into R. Iris is a data set that contains measurements for four features (sepal length, sepal width, petal length, and petal width) of three flower species (setosa, versicolor, and virginica). The four features are each a numeric variable while species is a factor (categorical) variable. 

If we would like to visualize the distribution of petal width values for three species then this we can consult the KylaJacksonPlotTypes.png file to see which visualization is best for two variables where one is numeric and the other categorical. Our options are strip chart, box plot, multiple histogram, and cumulative frequency distributions. We will use box plot for this example.

```{r}
boxplot(Petal.Width ~ Species, data = iris, ylab = "Petal Width", xlab = "Flower Species", main = "Variation in Petal Width Among Three Flower Species")
```

Feel free to practice with other visualizations and variables within the Iris data set before moving on to the next section!

HYPOTHESIS TESTING

Now that we are familiar with the tools to explore our data let's put them into practice to formulate and test a hypothesis. 

For practice let's test the hypothesis that treating with the drug pilocarpine increases spike rate.

To start it would be helpful to group the data based on the variables oyu are most interested in. This is a paired data set meaning we have only one sample, but measurements corresponding to before and after a treatment. Therfore we will want to be able to group based on the type of treatment and the time of the recording. In this case that would mean creating groups for the pilocarpine drug trials for the control recordings (before drug treatment) and the experimental recordings (after drug treatment).

This concept of grouping can be accomplished using the "subset" function. Subsetting allows you to establish your own subsets within the data that you can reference within tests. 

```{r}
pilocarpine_control <- subset(Neuron_Recording_Data, Drug=="Pilocarpine")$Control_Spikes
View(pilocarpine_control)
```

```{r}
pilocarpine_experimental <- subset(Neuron_Recording_Data, Drug=="Pilocarpine")$Experimental_Spikes
View(pilocarpine_experimental)
```

Now that we have our subsets of interest, lets go ahead and visualize them to get an idea of their distributions visually. This can be done using the "simple.eda" function which will construct a few different graphs portraying the data in different ways. 

```{r}
simple.eda(pilocarpine_control)
simple.eda(pilocarpine_experimental)
```

In order to proceed with testing this data, we need to first assess if it is normally distributed. This will dictate which types of tests are applicable. The shapiro.test function allows you to test the hypothesis that the data is normally distributed. It assumes a null hypothesis that the data follows a normal distribution and by including "$p.value" following our function we can extract just the important p-value information.   

```{r}
shapiro.test(pilocarpine_control)$p.value
shapiro.test(pilocarpine_experimental)$p.value
```

Since both of our p-values are greater than .05 we fail to reject the null hypothesis that the data is normally distributed. Therefore we can now proceed with statistical tests designed to investigate normally distributed data sets.

Had our sample not been normally distributed we would have to select different statistical tests, but I will elaborate on that more later. 

Ultimately we will be testing the null hypothesis that there is no difference in the spike rate before and after the application of the drug pilocarpine. This can be written more concisely as:

H0: Spike Rate (after) - Spike Rate(before) = 0

Our alternative hypothesis is that pilocarpine increased spike rate or rather:

HA: Spike Rate (after) - Spike Rate(before) > 0

A paired t-test will allow us to probe these hypotheses using the "t.test"" function since the sample was found to be normally distributed. You must specify the pre and post measurements of the paired sample that you will be comparing, followed by "paired" so that it knows not to treat the data as two independent samples, and finally you ust specify the alternative hypothesis as either "two.sided" - to test if the pre and post readings are different without specifying the direction of the difference, "greater" - as we will in this case to test if the post readings are greater than the pre readings, or "less" - to test if the post readings are less than the pre readings.

```{r}
t.test(pilocarpine_experimental,pilocarpine_control, paired = TRUE, alternative = "greater")
```

***Had the data instead not been found to be normally distributed from our shapiro test we would have to instead perform a wilcox rank sum test, this test is non parametric meaning it is robust to non-normal data sets.

We see from the output of our paired t-test that the p-value is less than .05. This leads us to reject our null hypothesis that the drug had no effect on spike rate. Therfore we can state that the drug pilocarpine significantly increased spike rate. 

We can visualize this impact of the drug with a box plot depicting the difference betwee pre and post drug administration recordings. 

If we put in both our experimental and our control subsets as our data for the boxplot we would get two separate boxplots, one for each subset that wouldbe identicel to the ones from their respective initial EDA's. This is not what we want as these are not two independent samples, but rather a paired data set from a single sample. To visualize the DIFFERENCE between the paired recordings let's create a single variable representing the differences. 

```{r}
Spike_rate_change <- pilocarpine_experimental - pilocarpine_control
```

Now we can create a boxplot of these differences with the following code:

```{r}
boxplot(Spike_rate_change,ylim = c(-10,40), cex.lab = 1.4, xlab = "Pre/Post Pilocarpine", ylab = "Change in Spike Rate (spikes/sec)")
abline(h=0, lty = 3, lwd = 3)
```

To recap the framework outlined here will allow you to proceed from selecting a hypotheses of interest, subsetting the data based on the variables you are most interested in, visualizing those subsets and further assessing their normality, selecting the correct statistical test, and finally interpretting the results of that test with regards to your initial hypotheses.


****IN this hypothesis testing section we took you through how to test a given hypothesis. Now its your turn! Develop your own hypothesis and refer back to this example to guide your investigation of it. You could potentiall pick a different drug for instacne and see if it too alters the spike rate significantly!

---Acknowledgments--- 
Ian Vogel completed the R Basics and Hypothesis Testing sections.
Julie Berhane completed the Data Exploration and Visualization sections.
Additional edits made by each author to tailor the tutorial to NEUR 253 (Ian Vogel) and ANBE 296 (Julie Berhane).