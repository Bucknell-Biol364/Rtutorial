---
title: "Rtutorial"
output: html_document
author: "Ian Vogel, Julie Berhane"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

R Tutorial for Animal Behavior 266 and Neuroscience 253 at Bucknell University

BASIC - Ian
Introduction of what R is
Creating csv file for data
Create a folder on your local device
Save csv file into this folder
Set folder as working directory
Read csv file into R

R Basics:

Welcome to R for neuro 253!

R is a program for data science. It is an extremely useful tool for manipulating, visualizing, and investigating data. It will allow you to explore data to formulate hypotheses and then employ statistical tests to assess them.  

Just to give you the lay of the land, the window you are reading this in is where you will write text and code to work with data. Below is the console where code is executed. In the upper right is a window that will show your environment and history of edits. Finally in the bottom right is where you will find something called your working directory. 

Before we can do any data visualizations or analyses we will first need a place to store all of our data sets as well as the code we write to work with them.This place is called your working directory and you will need to create one for this exercise. 

To create a working directory you should first create a folder in a location on this computer that you can readily access. Next select the "session" tab located above this window and click "set working directory" and select "choose directory" from within the drop down menu. Within file explorer that will appear, navigate to where you stored the new folder you just created. Select the folder and click ok to set it as your working directory.

Next we want some data to play with! There are a ton of great sample data sets avalable online and even a few built in ones here in R, but for this course we will be using a data set from a lab similar to those you will be performing this semester. The data set "Neuro_253_Cleaned_Data" is a series of extracellular neuronal recordings from crayfish. It includes recordings of "spikes" which refers to the firing of the neurons in the units of action potentials per second measured both before and after treatment with drugs. 

This file is in the .csv format, an abbreviation for "comma separated values". You can create such a file from an excel document. After entering and organizing data in an excel spreadsheet you can "save as" with the file extension .csv. You will need to save your data sets within the folder you have set as your working directory so that it will be accessible within R. 

Once the Neuro_253_Cleaned_Data.csv file is in your working directory you will want to read it into the R session as a data frame. A data frame is a table of information that can be of various types which is convenient when data are not of uniform classes. The read.csv function automatically takes the file name you provide in quotes and creates a data frame named whatever you provide to the left of the command containing the information from your data file. Below you can see the code in action.



```{r Import Data}
Neuron_Recording_Data <- read.csv("Neuro_253_Cleaned_Data.csv")
View(Neuron_Recording_Data)
```

You now have a data set in R that is ready to be manipulated using the commands within this document. 

DATA EXPLORATION
You can begin exploring your data set by looking at the head (first five rows) and tail (last five rows) of your dataset. You can also view the full dataset by typing the name of the file and running that line of code.

Another issue you should determine from the start is if there are missing values in your dataset. If it is a dataset you have collected yourself this may be easy to determine if the dataset was small enough, but when using large data or data from another source you should make sure to check this. You can check for missing values (often represented as NA, but could potentially be -999 or something else) and then choose if you want to keep or omit them from the data set.

```{r}
# Display full dataset
NameOfYourRawDataFile
# Display first five rows only (useful for large datasets)
head(NameOfYourRawDataFile)
# Display last five rows only
tail(NameOfYourRawDataFile)
# Count missing values
sum(is.na(NameOfYourRawDataFile))
# Exclude observations with missing values
NameOfYourRawDataFile <- na.omit(NameOfYourRawDataFile)
```

The layout of the data can also have consequences for how well your R code works. R treats the data as if each row is from a single observation. For example, let's consider you were determining if there is a difference test scores in two separate classes. The data could be set up such that Class A is the title of the first column and below it are the test scores of 10 different students, and Class B is the title of the second column with it's students test scores below it. R woudl treat the first row as one observation when it is actually two, one test score from class A and one score from test B. Stacked data will have the title of the first column be Grades or Values, and the title of the second column Class. So the rows would then be set up as a single observation for a single student, showing their test score and the Class (A or B) that they belong to.

You can use the stack() function to stack all of your data or select for certain values to stack.

```{r}
NewDataName <- stack(OldDataName)
NewDataName <- stack(Variable1, select=c("Variable 2 Value a", "Variable 2 Value b"))
```

Once the data is set up how you want, you can look as some quick summary statistics. The mean and middle are different ways to look at the middle of the data set. You can also look at the dispersion of the data with the range, standard deviation, variance, standard error of the mean, and interquartile range.

```{r}
# mean
mean(variable)
# median
median(variable)
# range
range(variable)
# standard deviation
sd(variable)
# variance
var(variable)
# standard error of the mean. sqrt is the square root function. length returns the number of values in that variable
SEM = sd(variable)/sqrt(length(variable))
# interquartile range
IQR(variable)
```

When working with quantitative data you should check to see if it is normally distributed. This is important when you eventually decide which statistical test to use as some of them assume that the data is normal. 

You can check to see if the data is normal using the simple.eda function in the UsingR package. The simple.eda function will show you a histogram, boxplot, and normal Q-Q plot of the data. 

A histogram will give you an idea of the distrbution of your variable by showing the frequency of each observed value. The graph should follow a bell shaped curve if the data is normal. 

A boxplot shows the spread of the data and if there are any outliers. Normal data should only have 1 outlier for every 100 data points. So if you have a small data set but there are a lot of outliers (points that extend beyond the box and whiskers) in your boxplot it may not be normally disributed. 

Q-Q plots are used to determine if the distribution of the data matches a normally distributed data set. If the points do not closely lie on the regression the plot creates then it is most likely not normally distributed.

```{r}
install.packages("UsingR")
library(UsingR)
simple.eda()
```

The Shapiro Wilks test is a way to statistically test if data is normal. This can be done using the shapiro.test function. The null hypothesis for this test is that the data is normal, so a p value less than 0.05 will reject the null and assume that the data is not normal. Therefore, if the p value returned is greater than 0.05 then the data is normal. This function will return some output that you don't need to look at. To focus on the p value use shapiro.test()$p.value.

```{r}
shapiro.test()$p.value
```

If the data is not normal, you may want to see if the data can be transformed and fit to be normal. This can be done by taking the log of the data. You can take the natural log and the log base 10 of the data and then run a Shapiro Wilks test to see if it is now normal.

```{r}
# natural log
shapiro.test(log(dataset$variable))
# log base 10
shapiro.test(log10(dataset$variable))
```

If the data still isn't normal after transforming, then you should consider it not normal and then use non-parametric tests during your statistical analysis. Parametric tests (such as the t test and ANOVA) are those that assume that data is normal. These are not suitable for analyzing non-normal data and you will have to determine which non-parametric test is appropriate when you begin to run statistical tests on your data.

DATA VISUALIZATION
Although statistical tests will determine if there is a statistically significant relationship between your variables, it can be easier to convey this information visually for the reader.

There are many ways to visually represent data, but you should select one based on what is appropriate for the types of variables you are testing as well as how many variables. The variable you are testing will either be numeric or categorical (factor). You should also make decisions based on if your numeric variable is discrete or continuous.

Below is the code for for different graphs you can make in R.

```{r}
# Bar graph/plot
Height = c(mean(x),mean(y),mean(z))
barplot(Height, xlab = "My x-axis label", ylab = "My y-axis label", names = c("Name of x", "Name of y", "Name of z"), cex.lab = 1.5)
abline(h=0) # why is this here with it equal to 0?

# Grouped bar graph
install.packages("ggplot2")
library(ggplot2)
ggplot(data, aes(fill=condition, y=value, x=specie))
geom_bar(position="dodge", stat="identity")

# Histogram
hist(variable, cex.lab = 1.5, main ="Main Title", breaks = 34, ylim = c(0,10))
abline(v = mean(variable), lwd = 5)
par(mfrow = c(1,1)) # I don't know what this means

# Scatterplot
plot(dataset, xlim = c(0.10), ylim = c(0,10), xlab = "My x-axis label", ylab = "My y-axis label", main = "Custom Title", pch = 16, cex, 1.2, cex.lab = 1.5)

# Mosaic plot
ggplot(data = dataset) +
   geom_mosaic(aes(x = product(variable1, variable2), fill=variable1), na.rm=TRUE) + 
  # OR
mosaicplot(~ quantVariable + factorVariable, data = dataset, color = TRUE)

# Strip chart
ggplot(dataset) +
  aes(x = as.numeric(factorVariable), y = quantVariable) +
  geom_jitter(aes(color = factor(otherVariable)), na.rm=TRUE) +
  theme_cowplot() +
  theme(legend.position = "right") +
  labs(color="Legend title") + 
  xlab("My x-axis label for factor variable") + 
  ylab("My y-axis label for quant variable")

# Box plot, page 72 in textbook. x,y,z are different conditions of one variable
boxplot(x,y,z, names = c("Name1","Name2","Name3"), xlab = "My x-axis label", ylab = "My y-axis label", cex.lab = 1.5, ylim = c(0,max(c(x,y,z))))
```


HYPOTHESIS TESTING

Now that we are familiar with the tools to explore our data let's put them into practice to formulate and test a hypothesis. 

For practice let's test the hypothesis that treating with the drug pilocarpine increases spike rate.

To start it would be helpful to group the data based on the variables oyu are most interested in. This is a paired data set meaning we have only one sample, but measurements corresponding to before and after a treatment. Therfore we will want to be able to group based on the type of treatment and the time of the recording. In this case that would mean creating groups for the pilocarpine drug trials for the control recordings (before drug treatment) and the experimental recordings (after drug treatment).

This concept of grouping can be accomplished using the "subset" function. Subsetting allows you to establish your own subsets within the data that you can reference within tests. 

```{r}
pilocarpine_control <- subset(Neuron_Recording_Data, Drug=="Pilocarpine")$Control_Spikes
View(pilocarpine_control)
```


```{r}
pilocarpine_experimental <- subset(Neuron_Recording_Data, Drug=="Pilocarpine")$Experimental_Spikes
View(pilocarpine_experimental)
```


Now that we have our subsets of interest, lets go ahead and visualize them to get an idea of their distributions visually. This can be done using the "simple.eda" function which will construct a few different graphs portraying the data in different ways. 

```{r}
simple.eda(pilocarpine_control)
simple.eda(pilocarpine_experimental)
```

In order to proceed with testing this data, we need to first assess if it is normally distributed. This will dictate which types of tests are applicable. The shapiro.test function allows you to test the hypothesis that the data is normally distributed. It assumes a null hypothesis that the data follows a normal distribution and by including "$p.value" following our function we can extract just the important p-value information.   

```{r}
shapiro.test(pilocarpine_control)$p.value
shapiro.test(pilocarpine_experimental)$p.value
```

Since both of our p-values are greater than .05 we fail to reject the null hypothesis that the data is normally distributed. Therefore we can now proceed with statistical tests designed to investigate normally distributed data sets.

Had our sample not been normally distributed we would have to select different statistical tests, but I will elaborate on that more later. 

Ultimately we will be testing the null hypothesis that there is no difference in the spike rate before and after the application of the drug pilocarpine. This can be written more concisely as:

H0: Spike Rate (after) - Spike Rate(before) = 0

Our alternative hypothesis is that pilocarpine increased spike rate or rather:

HA: Spike Rate (after) - Spike Rate(before) > 0

A paired t-test will allow us to probe these hypotheses using the "t.test"" function since the sample was found to be normally distributed. You must specify the pre and post measurements of the paired sample that you will be comparing, followed by "paired" so that it knows not to treat the data as two independent samples, and finally you ust specify the alternative hypothesis as either "two.sided" - to test if the pre and post readings are different without specifying the direction of the difference, "greater" - as we will in this case to test if the post readings are greater than the pre readings, or "less" - to test if the post readings are less than the pre readings.


```{r}
t.test(pilocarpine_experimental,pilocarpine_control, paired = TRUE, alternative = "greater")
```

***Had the data instead not been found to be normally distributed from our shapiro test we would have to instead perform a wilcox rank sum test, this test is non parametric meaning it is robust to non-normal data sets.


We see from the output of our paired t-test that the p-value is less than .05. This leads us to reject our null hypothesis that the drug had no effect on spike rate. Therfore we can state that the drug pilocarpine significantly increased spike rate. 

We can visualize this impact of the drug with a box plot depicting the difference betwee pre and post drug administration recordings. 

If we put in both our experimental and our control subsets as our data for the boxplot we would get two separate boxplots, one for each subset that wouldbe identicel to the ones from their respective initial EDA's. This is not what we want as these are not two independent samples, but rather a paired data set from a single sample. To visualize the DIFFERENCE between the paired recordings let's create a single variable representing the differences. 

```{r}
Spike_rate_change <- pilocarpine_experimental - pilocarpine_control
```

Now we can create a boxplot of these differences with the following code:

```{r}
boxplot(Spike_rate_change,ylim = c(-10,40), cex.lab = 1.4, xlab = "Pre/Post Pilocarpine", ylab = "Change in Spike Rate (spikes/sec)")
abline(h=0, lty = 3, lwd = 3)
```

To recap the framework outlined here will allow you to proceed from selecting a hypotheses of interest, subsetting the data based on the variables you are most interested in, visualizing those subsets and further assessing their normality, selecting the correct statistical test, and finally interpretting the results of that test with regards to your initial hypotheses.

Now try it out with a different drug and hypothesis from the initial data set!